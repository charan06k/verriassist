{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48ddce6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T16:44:30.580160Z",
          "iopub.status.busy": "2025-03-20T16:44:30.579725Z",
          "iopub.status.idle": "2025-03-20T16:50:51.582970Z",
          "shell.execute_reply": "2025-03-20T16:50:51.582080Z",
          "shell.execute_reply.started": "2025-03-20T16:44:30.580116Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": false,
          "start_time": "2025-03-20T18:35:58.157199",
          "status": "running"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# First uninstall problematic packages\n",
        "!pip uninstall -y protobuf google-api-core async-timeout\n",
        "\n",
        "# Install core requirements with version pinning\n",
        "!pip install -q --force-reinstall \\\n",
        "    torch==2.5.1 \\\n",
        "    torchaudio==2.5.1 \\\n",
        "    torchvision==0.20.1 \\\n",
        "    tensorflow==2.17.0 \\\n",
        "    numpy==1.26.4 \\\n",
        "    fsspec==2024.10.0 \\\n",
        "    protobuf==3.20.3 \\\n",
        "    google-api-core==2.16.2 \\\n",
        "    async-timeout==4.0.2 \\\n",
        "    scikit-learn==1.3.2 \\\n",
        "    matplotlib==3.8.0 \\\n",
        "    absl-py==1.4.0 \\\n",
        "    pandas==2.2.2 \\\n",
        "    transformers>=4.39.0\n",
        "\n",
        "# Install remaining requirements\n",
        "!pip install -q trl bitsandbytes>=0.39.0 accelerate\n",
        "\n",
        "# Special handling for Colab notebook compatibility\n",
        "!pip install -q --upgrade --ignore-installed notebook==6.5.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4182ca47",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T17:35:40.659793Z",
          "iopub.status.busy": "2025-03-20T17:35:40.659492Z",
          "iopub.status.idle": "2025-03-20T17:35:51.395269Z",
          "shell.execute_reply": "2025-03-20T17:35:51.394258Z",
          "shell.execute_reply.started": "2025-03-20T17:35:40.659771Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install -q trl\n",
        "!pip install -q bitsandbytes>=0.39.0\n",
        "!pip install -q accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6c2b49",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T16:51:01.776910Z",
          "iopub.status.busy": "2025-03-20T16:51:01.776495Z",
          "iopub.status.idle": "2025-03-20T16:51:04.880262Z",
          "shell.execute_reply": "2025-03-20T16:51:04.879393Z",
          "shell.execute_reply.started": "2025-03-20T16:51:01.776871Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import importlib.metadata\n",
        "print(f\"bitsandbytes version: {importlib.metadata.version('bitsandbytes')}\")\n",
        "import transformers\n",
        "print(f\"Transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ff5500",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T16:51:04.881896Z",
          "iopub.status.busy": "2025-03-20T16:51:04.881586Z",
          "iopub.status.idle": "2025-03-20T16:51:05.275148Z",
          "shell.execute_reply": "2025-03-20T16:51:05.273997Z",
          "shell.execute_reply.started": "2025-03-20T16:51:04.881876Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Correct Configuration Registration\n",
        "from transformers import PretrainedConfig\n",
        "import torch.nn as nn  # Ensure this is imported\n",
        "\n",
        "class FalconMambaConfig(PretrainedConfig):\n",
        "    model_type = \"falcon_mamba\"\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dummy_param = True\n",
        "\n",
        "FalconMambaConfig.register_for_auto_class()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "356175fc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T16:51:05.276552Z",
          "iopub.status.busy": "2025-03-20T16:51:05.276208Z",
          "iopub.status.idle": "2025-03-20T16:51:17.776994Z",
          "shell.execute_reply": "2025-03-20T16:51:17.776120Z",
          "shell.execute_reply.started": "2025-03-20T16:51:05.276518Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Correct registration method\n",
        "FalconMambaConfig.register_for_auto_class()\n",
        "# Step 1: Import required libraries\n",
        "from datasets import load_dataset, Features, Value\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, DataCollatorForLanguageModeling, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c9217a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T16:51:17.778244Z",
          "iopub.status.busy": "2025-03-20T16:51:17.777958Z",
          "iopub.status.idle": "2025-03-20T16:51:17.890507Z",
          "shell.execute_reply": "2025-03-20T16:51:17.889739Z",
          "shell.execute_reply.started": "2025-03-20T16:51:17.778221Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b65ae0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T16:51:17.891705Z",
          "iopub.status.busy": "2025-03-20T16:51:17.891389Z",
          "iopub.status.idle": "2025-03-20T16:51:17.945428Z",
          "shell.execute_reply": "2025-03-20T16:51:17.944703Z",
          "shell.execute_reply.started": "2025-03-20T16:51:17.891670Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#testing if bitsadnbytes is wroking\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Try to create a 4-bit quantized layer to test compatibility\n",
        "try:\n",
        "    test_linear = bnb.nn.Linear4bit(10, 10)\n",
        "    print(\"4-bit quantization is supported on your system\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating 4-bit layer: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd225f9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T16:51:17.946506Z",
          "iopub.status.busy": "2025-03-20T16:51:17.946273Z",
          "iopub.status.idle": "2025-03-20T16:51:17.951041Z",
          "shell.execute_reply": "2025-03-20T16:51:17.950373Z",
          "shell.execute_reply.started": "2025-03-20T16:51:17.946486Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#checking the path of it\n",
        "import sys\n",
        "print(sys.path)\n",
        "import bitsandbytes\n",
        "print(bitsandbytes.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd10d42",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T18:15:06.386033Z",
          "iopub.status.busy": "2025-03-20T18:15:06.385669Z",
          "iopub.status.idle": "2025-03-20T18:15:06.405465Z",
          "shell.execute_reply": "2025-03-20T18:15:06.404413Z",
          "shell.execute_reply.started": "2025-03-20T18:15:06.386003Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 2: Improved preprocessing functions\n",
        "def extract_module_description(code):\n",
        "    \"\"\"Extract the high-level description of the module to use as prompt.\n",
        "    This looks for header comments that describe the entire module's purpose.\"\"\"\n",
        "    \n",
        "    # Try to find block comments at the top that describe the module purpose\n",
        "    module_purpose_match = re.search(r'/\\*+[\\s\\*]*(.*?)\\*+/', code, re.DOTALL)\n",
        "    if module_purpose_match:\n",
        "        # Clean up the comment text\n",
        "        comment_text = re.sub(r'^\\s*\\*+\\s*', '', module_purpose_match.group(1), flags=re.MULTILINE)\n",
        "        comment_text = re.sub(r'\\s*Copyright.*$', '', comment_text, flags=re.MULTILINE | re.IGNORECASE)\n",
        "        comment_text = re.sub(r'\\s*License.*$', '', comment_text, flags=re.MULTILINE | re.IGNORECASE)\n",
        "        return comment_text.strip()\n",
        "    \n",
        "    # Try to find consecutive single-line comments at the top that describe purpose\n",
        "    purpose_comments = []\n",
        "    lines = code.split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('//'):\n",
        "            purpose_comments.append(line[2:].strip())\n",
        "        # Stop when we hit non-comment content\n",
        "        elif line and not line.startswith('/*') and not line.startswith('*/'):\n",
        "            break\n",
        "    \n",
        "    if purpose_comments:\n",
        "        return '\\n'.join(purpose_comments)\n",
        "    \n",
        "    # If no appropriate comments found, extract the module name at least\n",
        "    module_name_match = re.search(r'module\\s+(\\w+)', code)\n",
        "    if module_name_match:\n",
        "        return f\"Create a Verilog module named {module_name_match.group(1)}\"\n",
        "    \n",
        "    return \"Generate Verilog code for the specified functionality.\"\n",
        "\n",
        "def remove_module_description(code):\n",
        "    \"\"\"Remove the high-level description comment from the code,\n",
        "    while preserving all other code-specific comments.\"\"\"\n",
        "    \n",
        "    # Remove block comments at the top that describe module purpose\n",
        "    code = re.sub(r'^/\\*+[\\s\\*]*.*?\\*+/', '', code, flags=re.DOTALL, count=1)\n",
        "    \n",
        "    # Remove consecutive single-line comments at the top that describe purpose\n",
        "    lines = code.split('\\n')\n",
        "    start_idx = 0\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.strip().startswith('//'):\n",
        "            start_idx = i + 1\n",
        "        elif line.strip():  # Found non-empty, non-comment line\n",
        "            break\n",
        "    \n",
        "    return '\\n'.join(lines[start_idx:]).strip()\n",
        "\n",
        "def clean_verilog_code(code):\n",
        "    \"\"\"Clean and format the Verilog code while preserving code-related comments.\"\"\"\n",
        "    if not isinstance(code, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove copyright/license notices\n",
        "    code = re.sub(r'/\\*.*?(copyright|license).*?\\*/', '', code, flags=re.DOTALL | re.IGNORECASE)\n",
        "    \n",
        "    # Preserve indentation and formatting for readability\n",
        "    code = code.strip()\n",
        "    \n",
        "    # Remove excessive blank lines\n",
        "    code = '\\n'.join([line[:100] for line in code.split('\\n')])  # Limit line length\n",
        "    return code[:4000]  # Hard limit of 4000 characters\n",
        "\n",
        "def is_valid_verilog(code):\n",
        "    \"\"\"Check if the code contains valid Verilog structure.\"\"\"\n",
        "    if not isinstance(code, str):\n",
        "        return False\n",
        "    \n",
        "    # Check for module-endmodule structure\n",
        "    return bool(re.search(r'\\bmodule\\b.*?\\bendmodule\\b', code, flags=re.DOTALL))\n",
        "\n",
        "# Step 3: Improved dataset processing function\n",
        "def preprocess_verilog_dataset(batch):\n",
        "    \"\"\"Process raw Verilog code into prompt-completion pairs.\"\"\"\n",
        "    prompts = []\n",
        "    completions = []\n",
        "    \n",
        "    for raw_code in batch[\"code\"]:\n",
        "        if not is_valid_verilog(raw_code):\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            # Extract the high-level module description for the prompt\n",
        "            description = extract_module_description(raw_code)\n",
        "            \n",
        "            # Remove the high-level description but keep code-related comments\n",
        "            implementation = clean_verilog_code(raw_code)\n",
        "            \n",
        "            # Only include examples with meaningful descriptions and implementations\n",
        "            if description and len(description) > 10 and implementation:\n",
        "                prompts.append(description)\n",
        "                completions.append(implementation)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing example: {str(e)}\")\n",
        "            continue\n",
        "            \n",
        "    return {\n",
        "        \"prompt\": prompts,\n",
        "        \"completion\": completions\n",
        "    }\n",
        "#vadilation\n",
        "def validate_dataset_format(dataset):\n",
        "    \"\"\"Validate that the tokenized dataset has the correct format.\"\"\"\n",
        "    print(\"\\nValidating tokenized dataset format...\")\n",
        "    \n",
        "    # Check if the dataset has the expected columns\n",
        "    expected_columns = ['input_ids', 'attention_mask']\n",
        "    if not all(col in dataset.column_names for col in expected_columns):\n",
        "        print(f\"❌ Dataset is missing required columns. Found: {dataset.column_names}\")\n",
        "        return False\n",
        "    \n",
        "    # Check the first example\n",
        "    example = dataset[0]\n",
        "    print(f\"First example input_ids (first 10 tokens): {example['input_ids'][:10]}...\")\n",
        "    print(f\"First example attention_mask (first 10 tokens): {example['attention_mask'][:10]}...\")\n",
        "    \n",
        "    # Check if the input_ids are integers\n",
        "    if not all(isinstance(token_id, int) for token_id in example['input_ids'][:10]):\n",
        "        print(\"❌ input_ids are not integers\")\n",
        "        return False\n",
        "    \n",
        "    # Try to create a batch\n",
        "    from torch.utils.data import DataLoader\n",
        "    \n",
        "    # Create a data collator for tokenized inputs\n",
        "    data_collator = transformers.DefaultDataCollator()\n",
        "    \n",
        "    # Create a mini dataloader\n",
        "    mini_dataset = dataset.select(range(min(2, len(dataset))))\n",
        "    dataloader = DataLoader(\n",
        "        mini_dataset, \n",
        "        batch_size=2, \n",
        "        collate_fn=data_collator\n",
        "    )\n",
        "    \n",
        "    # Try to get a batch\n",
        "    try:\n",
        "        batch = next(iter(dataloader))\n",
        "        print(\"✅ Successfully created a batch!\")\n",
        "        print(f\"Batch keys: {batch.keys()}\")\n",
        "        print(f\"Input_ids shape: {batch['input_ids'].shape}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to create batch: {str(e)}\")\n",
        "        return False\n",
        "# Step 4: Load and process the dataset\n",
        "def prepare_verilog_dataset(tokenizer):\n",
        "    \"\"\"Prepare and tokenize the dataset for training.\"\"\"\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"shailja/Verilog_GitHub\", split=\"train\")\n",
        "    dataset = dataset.rename_column(\"text\", \"code\")\n",
        "    \n",
        "    # Use a subset for faster development\n",
        "    subset = dataset.select(range(100))\n",
        "    \n",
        "    # Process dataset\n",
        "    processed_dataset = subset.map(\n",
        "        preprocess_verilog_dataset,\n",
        "        batched=True,\n",
        "        batch_size=10,\n",
        "        remove_columns=subset.column_names\n",
        "    )\n",
        "    \n",
        "    # Filter invalid examples\n",
        "    processed_dataset = processed_dataset.filter(\n",
        "        lambda x: x[\"prompt\"] and x[\"completion\"] and len(x[\"prompt\"]) > 10\n",
        "    )\n",
        "    \n",
        "    # Format for instruction tuning\n",
        "    def format_for_instruction_tuning(sample):\n",
        "        prompt = str(sample['prompt']).strip()\n",
        "        completion = str(sample['completion']).strip()\n",
        "        \n",
        "        # Limit text length if needed\n",
        "        if len(completion) > 3500:\n",
        "            completion = completion[:3500]\n",
        "        \n",
        "        formatted_text = f\"### Instruction:\\nGenerate Verilog code based on: {prompt}\\n\\n### Response:\\n{completion}\"\n",
        "        return {\"text\": formatted_text}\n",
        "    \n",
        "    formatted_dataset = processed_dataset.map(format_for_instruction_tuning)\n",
        "    \n",
        "    # Now tokenize the dataset\n",
        "    def tokenize_function(examples):\n",
        "        # Make sure padding is applied on the right side\n",
        "        tokenizer.padding_side = \"right\"  # ADDED: Explicitly set padding side\n",
        "        \n",
        "        # Tokenize the text\n",
        "        tokenized = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=4096,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None  # Return Python lists instead of tensors\n",
        "        )\n",
        "        \n",
        "        # ADDED: Check for attention mask issues\n",
        "        for i, mask in enumerate(tokenized[\"attention_mask\"]):\n",
        "            if sum(mask) == 0:\n",
        "                print(f\"Warning: Example {i} has an all-zero attention mask\")\n",
        "                # Print the text to help diagnose the issue\n",
        "                if i < len(examples[\"text\"]):\n",
        "                    print(f\"Text sample: {examples['text'][i][:100]}...\")\n",
        "        \n",
        "        return tokenized\n",
        "    \n",
        "    # Apply tokenization\n",
        "    tokenized_dataset = formatted_dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        # MODIFIED: Remove all non-tokenized columns\n",
        "        remove_columns=[\"text\", \"prompt\", \"completion\"]\n",
        "    )\n",
        "    \n",
        "    # Verify the tokenized dataset\n",
        "    print(f\"Tokenized dataset columns: {tokenized_dataset.column_names}\")\n",
        "    print(f\"Example tokenized entry: {tokenized_dataset[0]['input_ids'][:10]}... (truncated)\")\n",
        "    \n",
        "    # ADDED: Print additional diagnostic info\n",
        "    example = tokenized_dataset[0]\n",
        "    non_zero_tokens = sum(example['attention_mask'])\n",
        "    print(f\"Non-zero tokens in first example: {non_zero_tokens} out of {len(example['attention_mask'])}\")\n",
        "    \n",
        "    return tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6011a576",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T18:33:17.343725Z",
          "iopub.status.busy": "2025-03-20T18:33:17.343387Z",
          "iopub.status.idle": "2025-03-20T18:33:17.356067Z",
          "shell.execute_reply": "2025-03-20T18:33:17.355022Z",
          "shell.execute_reply.started": "2025-03-20T18:33:17.343700Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 5: Set up the model with LoRA for efficient fine-tuning\n",
        "# First, let's patch the validation function that's failing\n",
        "from transformers.utils.import_utils import is_bitsandbytes_available\n",
        "import transformers.quantizers.quantizer_bnb_4bit\n",
        "# Add PyTorch imports at the top of your code\n",
        "import torch.nn as nn  # Add this line\n",
        "from transformers.utils.import_utils import is_bitsandbytes_available\n",
        "import transformers.quantizers.quantizer_bnb_4bit\n",
        "# Force the availability check to return True\n",
        "def patched_is_bitsandbytes_available():\n",
        "    return True\n",
        "\n",
        "# Monkey patch the function\n",
        "transformers.utils.import_utils.is_bitsandbytes_available = patched_is_bitsandbytes_available\n",
        "transformers.quantizers.quantizer_bnb_4bit.is_bitsandbytes_available = patched_is_bitsandbytes_available\n",
        "\n",
        "# Now try loading your model\n",
        "# Step 5: Set up the model with LoRA for efficient fine-tuning\n",
        "def setup_model_for_training():\n",
        "    model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "    \n",
        "    # Configure quantization (switched to 8-bit)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "    )\n",
        "    \n",
        "    # Load model with quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        use_cache=False  # Disable cache from the start\n",
        "    )\n",
        "    \n",
        "    # Explicitly disable cache in config\n",
        "    model.config.use_cache = False\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    \n",
        "    # For DeepSeek Coder target modules\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]\n",
        "    \n",
        "    # Updated LoRA config\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=target_modules,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    \n",
        "    # Apply LoRA to the model\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    \n",
        "    # Verify trainable parameters\n",
        "    print(\"\\nTrainable parameters:\")\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "# Step 7: Set up the trainer\n",
        "def create_trainer(model, tokenizer, dataset):\n",
        "    # Use data collator that generates labels\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "        pad_to_multiple_of=8\n",
        "    )\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./verilog-model-output\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        max_grad_norm=0.3,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        optim=\"adamw_torch\",\n",
        "        remove_unused_columns=False,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    \n",
        "    # Create standard trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "# Step 8: Main training function\n",
        "def train_verilog_model():\n",
        "    \"\"\"Main function to train the Verilog model.\"\"\"\n",
        "    # Setup model and tokenizer first\n",
        "    print(\"Setting up model...\")\n",
        "    model, tokenizer = setup_model_for_training()\n",
        "    \n",
        "    # Prepare the dataset with tokenization\n",
        "    print(\"\\nLoading and preparing dataset...\")\n",
        "    tokenized_dataset = prepare_verilog_dataset(tokenizer)\n",
        "    \n",
        "    # Validate the dataset format\n",
        "    if not validate_dataset_format(tokenized_dataset):\n",
        "        print(\"Dataset validation failed. Please fix the formatting issues before training.\")\n",
        "        return None, None\n",
        "    \n",
        "    # Print dataset info\n",
        "    print(f\"\\nDataset size: {len(tokenized_dataset)} examples\")\n",
        "    \n",
        "    # Create trainer using your custom function\n",
        "    print(\"\\nCreating trainer...\")\n",
        "    trainer = create_trainer(model, tokenizer, tokenized_dataset)\n",
        "    \n",
        "    # Start training with error handling\n",
        "    print(\"\\nStarting training...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"Training completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Training error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "        # Try to save what we have\n",
        "        try:\n",
        "            trainer.save_model(\"./emergency_saved_model\")\n",
        "            print(\"Model saved despite error.\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"Could not save model after error: {str(save_error)}\")\n",
        "    \n",
        "    # Save the final model\n",
        "    print(\"\\nSaving model...\")\n",
        "    trainer.save_model(\"./verilog_model_final\")\n",
        "    \n",
        "    return model, tokenizer\n",
        "    \n",
        "# Step 9: Test the trained model\n",
        "def test_model(model, tokenizer):\n",
        "    # Merge LoRA weights first\n",
        "    model = model.merge_and_unload()\n",
        "    \n",
        "    verilog_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device_map=\"auto\",\n",
        "        truncation=True,  # Add this\n",
        "        max_length=1024  # Add this\n",
        "    )\n",
        "    \n",
        "    # Test prompts\n",
        "    test_prompts = [\n",
        "        \"This is the simplest form of inferring the simple/SRL(16/32)CE in a Xilinx FPGA.\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nTesting model with example prompts:\")\n",
        "    for prompt in test_prompts:\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        result = verilog_pipe(\n",
        "            f\"### Instruction:\\n{prompt}\\n\\n### Response:\",\n",
        "            max_length=768,\n",
        "            temperature=0.3,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True\n",
        "        )\n",
        "        print(\"Generated code:\")\n",
        "        print(result[0]['generated_text'])\n",
        "        print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9691b36c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T18:15:08.362925Z",
          "iopub.status.busy": "2025-03-20T18:15:08.362602Z",
          "iopub.status.idle": "2025-03-20T18:29:42.775484Z",
          "shell.execute_reply": "2025-03-20T18:29:42.774227Z",
          "shell.execute_reply.started": "2025-03-20T18:15:08.362896Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 10: Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    model, tokenizer = train_verilog_model()\n",
        "    \n",
        "    # Test the model\n",
        "    #test_model(model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f61bd1",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": null,
      "end_time": null,
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-03-20T18:35:55.563378",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
